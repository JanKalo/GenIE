{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenIE: Generative Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of Content\n",
    "1. [How to download the required artefacts?](#Download)\n",
    "2. [How to load the models?](#Loading-the-Models)\n",
    "3. [How to run inference?](#Inference)\n",
    "    - [Unconstrained Generation](#Unconstrained-Generation)\n",
    "    - [Constrained Generation](#Constrainted-Generation)\n",
    "    - [Extracting the Wikidata Disambiguated Triplet Sets](#Extracting-the-Wikidata-Disambiguated-Triplet-Sets)\n",
    "4. [Loading models and running inference with Hydra](#Loading-Models-and-Running-Inference-with-Hydra)\n",
    "5. [How to load and use the datasets?](#Loading-Datasets)\n",
    "6. Optional\n",
    "    1. [How to constraint the model with a custom set of strings?](#Constructing-Prefix-Tries-for-A-Custom-Set-of-Strings) \n",
    "    2. [Loading and Using the WikidataID2Name Dictionaries](#Loading-and-Using-the-WikidataID2Name-Dictionaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we release consists of:\n",
    "\n",
    "1. **Pre-trained Model(s)**\n",
    "    - Wiki-NRE (W): [Random Initialization](https://zenodo.org/record/6139236/files/genie_w.ckpt)\n",
    "    - Rebel (R): [Random Initialization](https://zenodo.org/record/6139236/files/genie_r.ckpt) – [Pretrained Language Model](https://zenodo.org/record/6139236/files/genie_plm_r.ckpt) – [Pretrained Entity Linker (GENRE)](https://zenodo.org/record/6139236/files/genie_genre_r.ckpt)\n",
    "    - Rebel + Wiki-NRE (R+W): [Random Initialization](https://zenodo.org/record/6139236/files/genie_rw.ckpt)\n",
    "2. [**Prefix Trees (tries) for Constrained Generation**](https://zenodo.org/record/6139236/files/tries.zip)\n",
    "    - relation trie\n",
    "    - entity trie\n",
    "3. **Datasets** \\[Not required for inference\\] \n",
    "    - [Rebel](https://zenodo.org/record/6139236/files/rebel.zip)\n",
    "    - [FewRel](https://zenodo.org/record/6139236/files/fewrel.zip)\n",
    "    - [Wikipedia-NRE](https://zenodo.org/record/6139236/files/wikipedia_nre.zip)\n",
    "    - [Geo-NRE](https://zenodo.org/record/6139236/files/geo_nre.zip)\n",
    "4. [**World Definitions**](https://zenodo.org/record/6139236/files/world_definitions.zip) \\[Not required for inference\\] \n",
    "5. **Mapping between Unique Names and Wikidata Identifiers** ([used by GenIE](https://zenodo.org/record/6139236/files/surface_form_dicts.zip), [full snapshot](https://zenodo.org/record/6139236/files/surface_form_dicts_from_snapshot.zip)) \\[Optional. Necessary for processing data\\] \n",
    "    - relation name to wikidata ID (and vice-versa)\n",
    "    - entity name to wikidata ID (and vice-versa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the data by executing the <code>download_data.sh</code> script. If you want to omit some files, comment out parts of the code.\n",
    "\n",
    "Alternatively, you can access the data [here](https://zenodo.org/record/6139236#.YhJdiJPMJhH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using a different directory for your data, update the path below\n",
    "DATA_DIR=\"../data\"\n",
    "\n",
    "# To download the data uncomment and run the following line\n",
    "# !bash ../download_data.sh $DATA_DIR\n",
    "\n",
    "# If your working directory is not the GenIE fodler, include the path to it in your PATH variable to make the library available\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the Model\"\"\"\n",
    "from genie.models import GeniePL\n",
    "\n",
    "ckpt_name = \"genie_r.ckpt\"\n",
    "path_to_checkpoint = os.path.join(DATA_DIR, 'models', ckpt_name)\n",
    "model = GeniePL.load_from_checkpoint(checkpoint_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the Prefix Tries\"\"\"\n",
    "from genie.constrained_generation import Trie\n",
    "\n",
    "# Large schema tries (correspond to Rebel; see the paper for details) \n",
    "entity_trie_path = os.path.join(DATA_DIR, \"tries/large/entity_trie.pickle\")\n",
    "entity_trie = Trie.load(entity_trie_path)\n",
    "\n",
    "relation_trie_path = os.path.join(DATA_DIR, \"tries/large/relation_trie.pickle\")\n",
    "relation_trie = Trie.load(relation_trie_path)\n",
    "\n",
    "large_schema_tries = {'entity_trie': entity_trie, 'relation_trie': relation_trie}\n",
    "\n",
    "# Small schema tries (correspond to Wiki-NRE; see the paper for details) \n",
    "entity_trie_path = os.path.join(DATA_DIR, \"tries/small/entity_trie.pickle\")\n",
    "entity_trie = Trie.load(entity_trie_path)\n",
    "\n",
    "relation_trie_path = os.path.join(DATA_DIR, \"tries/small/relation_trie.pickle\")\n",
    "relation_trie = Trie.load(relation_trie_path)\n",
    "\n",
    "small_schema_tries = {'entity_trie': entity_trie, 'relation_trie': relation_trie}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a prefix trie for your custom set of strings see [this section](#Constructing-a-Prefix-Tries-for-A-Custom-Set-of-Strings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inference use the `model.sample` function. \n",
    "\n",
    "Under the hood, **GenIE** uses the HuggingFace's generate function, thus it accepts the same generation parameters. By default, during inference the same generation parameters used by the model during are employed – they are the model's default – but you can override them in the call of the function, as shown in the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Prior to KTRK, Carson was an anchor for FOX-owned KSAZ in Phoenix, Arizona.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Unconstrained Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': ' <sub> KTRK, Carson <rel> headquarters location <obj> Phoenix, Arizona <et>',\n",
       "   'log_prob': -0.19589225947856903},\n",
       "  {'text': ' <sub> KTRK, Carson <rel> located in the administrative territorial entity <obj> Arizona <et> <sub> KSAZ <rel> headquarters location <obj> Phoenix, Arizona <et>',\n",
       "   'log_prob': -0.2037668377161026}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "override_models_default_hf_generation_parameters = {\n",
    "    \"num_beams\": 10,\n",
    "    \"num_return_sequences\": 2,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "    \"seed\": 123\n",
    "}\n",
    "\n",
    "output = model.sample(sentences, \n",
    "                      **override_models_default_hf_generation_parameters)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without constraints the model mistakenly assumes that \"KTRK, Carson\" is an entity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrainted Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To constrain the generation, set the `entity_trie` and the `relation_trie` arguments of the sample to the entity and relation trie,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Small Schema Constrainted Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': ' <sub> Fox Broadcasting Company <rel> located in the administrative territorial entity <obj> Arizona <et> <sub> Phoenix, Arizona <rel> capital of <obj> Arizona <et> <sub> Arizona <rel> capital <obj> Phoenix, Arizona <et>',\n",
       "   'log_prob': -0.43319371342658997},\n",
       "  {'text': ' <sub> Fox Broadcasting Company <rel> headquarters location <obj> Arizona <et> <sub> Phoenix, Arizona <rel> capital of <obj> Arizona <et> <sub> Arizona <rel> capital <obj> Phoenix, Arizona <et>',\n",
       "   'log_prob': -0.4518451988697052}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Small Schema Constrainted Generation\"\"\"\n",
    "\n",
    "override_models_default_hf_generation_parameters = {\n",
    "    \"num_beams\": 10,\n",
    "    \"num_return_sequences\": 2,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "    \"seed\": 123\n",
    "}\n",
    "\n",
    "output = model.sample(sentences, \n",
    "                      **small_schema_tries, \n",
    "                      **override_models_default_hf_generation_parameters)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the small schema constraints results with the model linking the \"KTRK\" entity to the [Fox Broadcasting Company](https://en.wikipedia.org/wiki/Fox_Broadcasting_Company). This is due to the fact that the correct entity is missing from the \"small\" schema. \n",
    "\n",
    "Note that in comparison with the unconstrained generation, the (\"best\" eligible) predictions in this case are assigned a much lower score (log probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Large Schema Constrainted Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': ' <sub> KTRK <rel> headquarters location <obj> Phoenix, Arizona <et> <sub> KSAZ-TV <rel> headquarters location <obj> Phoenix, Arizona <et>',\n",
       "   'log_prob': -0.22215303778648376},\n",
       "  {'text': ' <sub> KTRK <rel> headquarters location <obj> Phoenix, Arizona <et>',\n",
       "   'log_prob': -0.22950957715511322}]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Large Schema Constrainted Generation\"\"\"\n",
    "\n",
    "override_models_default_hf_generation_parameters = {\n",
    "    \"num_beams\": 10,\n",
    "    \"num_return_sequences\": 2,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "    \"seed\": 123\n",
    "}\n",
    "\n",
    "output = model.sample(sentences,\n",
    "                      **large_schema_tries, \n",
    "                      **override_models_default_hf_generation_parameters)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, under the large schema constraint, the model links the subjects of the triplets to: 1) [KTRK](https://www.wikidata.org/wiki/Q6339093) – the disambiguation page of [KTRK-TV](https://en.wikipedia.org/wiki/KTRK-TV); 2) The TV station [KSAZ-TV](https://en.wikipedia.org/wiki/KTRK-TV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two examples illustrate how the generation for any of the **GenIE** models can be constrained with an arbitrary prefix tries. See how you can construct your custom prefix trie [here](#Constructing-a-Prefix-Tries-for-A-Custom-Set-of-Strings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Wikidata Disambiguated Triplet Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Textual Set of triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The textual predictions produced by the sampling function can be directly mapped to a subject-relation-object tuples by passing the argument <code>convert_to_triplets=True</code>. This requires that the <code>return_dict_in_generate=True</code> and adds an additional field <code>textual_triplets</code> to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WikidataID set of triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entity and relation names produced by GenIE are textual identifiers that can be uniquely translated to an element from Wikidata. To facilitate future reseach, and make the output of the models more useful, among the data that we release, we provide the mappings a mappiing between Wikidata IDs and entity/relation that cover all of the elements in a snapshot of the English Wikipedia. See [this](#Loading-and-Using-the-WikidataID2Name-Dictionaries) section for details.\n",
    "\n",
    "The sample function has the support for providing the predicted triplets as tuples of Wikidata IDs, translated according to name-to-ID mappings for entities and relations that need to be passed to the call. The <code>convert_to_triplets=True</code> must be set for this functionality, which adds an additional field <code>id_triplets</code> to the output. The example below shows how to leverage the provided dictionaries for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading mapping from: ../data/surface_form_dicts/ent_id2surface_form.jsonl\n",
      "Reading mapping from: ../data/surface_form_dicts/rel_id2surface_form.jsonl\n"
     ]
    }
   ],
   "source": [
    "from genie.datamodule.utils import WikidataID2SurfaceForm\n",
    "\n",
    "# Entity Mapping\n",
    "ent_id2surface_info_path = os.path.join(DATA_DIR, \"surface_form_dicts\", \"ent_id2surface_form.jsonl\") # used in our experiments\n",
    "ent_mapping = WikidataID2SurfaceForm(ent_id2surface_info_path)\n",
    "ent_mapping.load()\n",
    "\n",
    "# Relation Mapping\n",
    "rel_id2surface_info_path = os.path.join(DATA_DIR, \"surface_form_dicts\", \"rel_id2surface_form.jsonl\") # used in our experiments\n",
    "rel_mapping = WikidataID2SurfaceForm(rel_id2surface_info_path)\n",
    "rel_mapping.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"The physicist Einstein was given a Nobel Prize.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': ' <sub> Albert Einstein <rel> award received <obj> Nobel Prize in Physics <et>',\n",
       "   'log_prob': -0.16297683119773865,\n",
       "   'textual_triplets': {('Albert Einstein',\n",
       "     'award received',\n",
       "     'Nobel Prize in Physics')},\n",
       "   'id_triplets': [['Q937', 'P166', 'Q38104']]},\n",
       "  {'text': ' <sub> Albert Einstein <rel> award received <obj> Nobel Prize in Physiology or Medicine <et>',\n",
       "   'log_prob': -0.2737625539302826,\n",
       "   'textual_triplets': {('Albert Einstein',\n",
       "     'award received',\n",
       "     'Nobel Prize in Physiology or Medicine')},\n",
       "   'id_triplets': [['Q937', 'P166', 'Q80061']]}]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Large Schema Constrainted Generation\"\"\"\n",
    "\n",
    "override_models_default_hf_generation_parameters = {\n",
    "    \"num_beams\": 10,\n",
    "    \"num_return_sequences\": 2,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "    \"seed\": 123\n",
    "}\n",
    "\n",
    "convert_to_triples = True\n",
    "surface_form_mappings = {'entity_name2id': ent_mapping.surface_form2id, 'relation_name2id': rel_mapping.surface_form2id}\n",
    "\n",
    "output = model.sample(sentences,\n",
    "                      convert_to_triplets=convert_to_triples,\n",
    "                      surface_form_mappings=surface_form_mappings,\n",
    "                      **large_schema_tries, \n",
    "                      **override_models_default_hf_generation_parameters)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading Models and Running Inference with Hydra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way to load the models (or data) is by using the package manage [Hydra](https://hydra.cc/). Below is a way of using Hydra in a jupyter notebook, but the library shines when used in scripts. See our training and evaluation code for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading a Model and Performing Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "import hydra\n",
    "\n",
    "configs_path = \"../configs\"\n",
    "config_name = \"config.yaml\"\n",
    "\n",
    "with hydra.initialize(config_path=configs_path):\n",
    "    config = hydra.compose(config_name=config_name, \n",
    "                           overrides=[f\"data_dir={DATA_DIR}/\",\n",
    "                                      f\"work_dir=../\",\n",
    "                                      f\"model=ckpt_genie\"\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = hydra.utils.instantiate(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': ' <sub> Albert Einstein <rel> award received <obj> Nobel Prize in Physics <et>',\n",
       "   'log_prob': -0.16297677159309387},\n",
       "  {'text': ' <sub> Albert Einstein <rel> award received <obj> Nobel Prize in Physiology or Medicine <et>',\n",
       "   'log_prob': -0.2737625539302826}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.sample(sentences, \n",
    "                      entity_trie=model.entity_trie, \n",
    "                      relation_trie=model.relation_trie, \n",
    "                      seed=123)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### With Hydra and PyTorch Lightning (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import hydra\n",
    "\n",
    "configs_path = \"../configs\"\n",
    "config_name = \"config.yaml\"\n",
    "\n",
    "\"\"\"\n",
    "datamodule -> rebel, wikipedia_nre, geo_nre, fewrel\n",
    "\"\"\"\n",
    "\n",
    "with hydra.initialize(config_path=configs_path):\n",
    "    config = hydra.compose(config_name=config_name, \n",
    "                           overrides=[f\"data_dir={DATA_DIR}/\", \n",
    "                                      f\"work_dir=../\",\n",
    "                                      f\"datamodule=wikipedia_nre\"\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datamodule\n",
    "datamodule = hydra.utils.instantiate(config.datamodule, tokenizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/wikipedia_nre/val_dataset.jsonl: 100%|█| 988/988 [00:00<00:00, 42936.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "980"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.setup(\"validate\")\n",
    "len(datamodule.data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 5,\n",
       " 'src': 'During these five years Mikhail Bukinik studied with Alfred von Glehn ( who was also the teacher of Gregor Piatigorsky ) at the Moscow Conservatory .',\n",
       " 'trg': ' <sub> Mikhail Bukinik <rel> educated at <obj> Moscow Conservatory <et>',\n",
       " 'non_formatted_wikidata_id_output': [['Q1930285', 'P69', 'Q215539']]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.data_val[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Raw Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from genie.datamodule.datasets import Rebel, WikipediaNRE, FewRel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data from /dlabdata1/josifosk/GenIE_repro/GenIE_private2/data/wikipedia_nre/val_dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "980"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Rebel -> train, val, test\n",
    "WikipediaNRE -> train, val, test and trip (which corresopnds to GeoNRE)\n",
    "FewRel -> test\n",
    "\"\"\"\n",
    "\n",
    "split='val'\n",
    "raw_data, dataset = WikipediaNRE.from_kilt_dataset(data_split=split, \n",
    "                                                 tokenizer=None, \n",
    "                                                 return_raw_data=True,\n",
    "                                                 matching_status=\"title\",\n",
    "                                                 relations_to_keep='../data/world_definitions/complete_relations.jsonl',\n",
    "                                                )\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 5,\n",
       " 'src': 'During these five years Mikhail Bukinik studied with Alfred von Glehn ( who was also the teacher of Gregor Piatigorsky ) at the Moscow Conservatory .',\n",
       " 'trg': ' <sub> Mikhail Bukinik <rel> educated at <obj> Moscow Conservatory <et>',\n",
       " 'non_formatted_wikidata_id_output': [['Q1930285', 'P69', 'Q215539']]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Constructing Prefix Tries for A Custom Set of Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genie.constrained_generation.trie import Trie, get_trie_from_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 2999.50it/s]\n"
     ]
    }
   ],
   "source": [
    "names = [\"place of birth\", \"father\", \"employer\"]\n",
    "output_folder_path = os.path.join(DATA_DIR, \"tries\") # If the output folder path is None, the trie won't be saved to disk\n",
    "tokenizer = None # If the tokenizer is set to None the function is loading GenIE's tokenizer by default. If you are using a different model, load the tokenizer specific to your model.\n",
    "\n",
    "trie = get_trie_from_strings(names, \n",
    "                             output_folder_path=output_folder_path, \n",
    "                             trie_name=\"myCustomTrie\", \n",
    "                             tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading and Using the WikidataID2Name Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import config\n",
    "\n",
    "from genie.datamodule.utils import WikidataID2SurfaceForm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading mapping from: ../data/surface_form_dicts/ent_id2surface_form.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Entity Mapping\n",
    "ent_id2surface_info_path = os.path.join(DATA_DIR, \"surface_form_dicts\", \"ent_id2surface_form.jsonl\") # used in our experiments\n",
    "# ent_id2surface_info_path = os.path.join(DATA_DIR, \"surface_form_dicts_from_snapshot\", \"ent_id2surface_form.jsonl\") # maximal mapping extracted from the English Wikipedia snapshot\n",
    "ent_mapping = WikidataID2SurfaceForm(ent_id2surface_info_path)\n",
    "ent_mapping.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading mapping from: ../data/surface_form_dicts/rel_id2surface_form.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Relation Mapping\n",
    "\n",
    "rel_id2surface_info_path = os.path.join(DATA_DIR, \"surface_form_dicts\", \"rel_id2surface_form.jsonl\") # used in our experiments\n",
    "# rel_id2surface_info_path = os.path.join(DATA_DIR, \"surface_form_dicts_from_snapshot\", \"rel_id2surface_form.jsonl\") # maximal mapping extracted from the English Wikipedia snapshot\n",
    "rel_mapping = WikidataID2SurfaceForm(rel_id2surface_info_path)\n",
    "rel_mapping.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Map IDs to Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ids_to_names(ids, mapping, allow_querying=False, allow_labels=False, invalid_tokens = set([\" <\"])):\n",
    "    \"\"\"For some items there might not be a matching in the current dictionary for several reasons, such as: \n",
    "    1) The item is a redirect, hence it will be resolved to another item; \n",
    "    2) The item has been added after the snapshot from which the dictionary was generated; \n",
    "    3) The entity doesn't have an english article associated with it.\n",
    "    Issues like the first and the second can be resolved by qurying Wikidata (setting allow_querying=True). \n",
    "    To resolve the third we might use the label of the item (items that are not associated with an english article, are often assigned an english label). However, labels are not necessarily unique and might lead to duplicates. To enable this set allow_labels=True\"\"\"\n",
    "    \n",
    "    id2title = {}\n",
    "    for _id in ids:\n",
    "        invalid = False\n",
    "\n",
    "        unq_name, _ = mapping.get_from_wikidata_id(_id, \n",
    "                                                   return_provenance=True,\n",
    "                                                   query_wikidata=allow_querying, \n",
    "                                                   allow_labels=allow_labels)\n",
    "\n",
    "        if unq_name is not None:\n",
    "            for token in invalid_tokens:\n",
    "                if token in unq_name:\n",
    "                    print(\"The name contains a special token found ->\", title)\n",
    "                    invalid = True\n",
    "                    break\n",
    "\n",
    "            if invalid:\n",
    "                continue\n",
    "\n",
    "            id2title[_id] = unq_name\n",
    "\n",
    "    print(\"Out of the original `{}` elements, `{}` were successfully mapped\".format(len(ids), len(id2title)))\n",
    "    return id2title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of the original `8` elements, `5` were successfully mapped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Q9659': 'A',\n",
       " 'Q41746': 'Achilles',\n",
       " 'Q20127832': 'Achilles Stakes',\n",
       " 'Q4673749': 'Achilles Rink',\n",
       " 'Q4673754': 'Achilles Rizzoli'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = list(ent_mapping.id2surface_form.keys())[:5]\n",
    "invalid_ids = [\"randomID\", \"randomID2\", \"randomID3\"]\n",
    "ids += invalid_ids\n",
    "\n",
    "map_ids_to_names(ids, ent_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Map Names To IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_names_to_ids(names, mapping):\n",
    "    surface_form2id = mapping.surface_form2id\n",
    "    \n",
    "    name2id = {name: surface_form2id[name] for name in names if name in surface_form2id}\n",
    "    \n",
    "    print(\"Out of the original `{}` elements, `{}` were successfully mapped\".format(len(names), len(name2id)))\n",
    "    return name2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of the original `6` elements, `3` were successfully mapped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'place of birth': 'P19', 'father': 'P22', 'employer': 'P108'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [\"place of birth\", \"father\", \"employer\"]\n",
    "invalid_names = [\"randomName\", \"randomName2\", \"randomName3\"]\n",
    "names += invalid_names\n",
    "\n",
    "map_names_to_ids(names, rel_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genie_final2",
   "language": "python",
   "name": "genie_final2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
